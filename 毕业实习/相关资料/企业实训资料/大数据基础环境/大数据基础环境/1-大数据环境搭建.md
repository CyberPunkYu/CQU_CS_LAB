# PART1. 大数据环境准备

## 1.1 关闭防火墙

```shell
systemctl status firewalld.service  #查看防火墙运行状态
systemctl stop firewalld.service   #关闭防火墙
systemctl disable  firewalld.service  #禁止开机自启
```

<img src=".\image\image-20220628083518378.png" alt="image-20220628083518378" style="zoom:80%;" />



## 1.2 关闭selinux

```shell
vi /etc/selinux/config
```

<img src=".\image\image-20220628084401281.png" alt="image-20220628084401281" style="zoom:50%;" />



## 1.3 SSH免密登录

```shell
ssh-keygen -t rsa    #四下回车键
ssh-copy-id ip
```

<img src=".\image\image-20220628084715210.png" alt="image-20220628084715210" style="zoom:50%;" />



## 1.4 JDK安装

```shell
rpm -qa | grep java #查看系统中是否有openjdk，有的话，则使用[rpm -e 文件名]命令进行删除

yum install lrzsz -y #安装终端与windows交互的插件
sudo rz  #上传命令

#创建用于安装软件的目录[app],用于存放安装包的目录[tools]
cd /home
mkdir app
mkdir tools
```

![image-20220628092445970](.\image\image-20220628092445970.png)

```shell
tar -zxvf ./jdk-8u171-linux-x64.tar.gz -C /home/app/
cd /home/app
mv jdk1.8.0_171/ jdk1.8  #重命名文件
vim /etc/profile  #修改环境变量
写入内容：
#JAVA_HOME
export JAVA_HOME=/home/app/jdk1.8
export PATH=:$JAVA_HOME/bin:$PATH

source /etc/profile #保存退出后生效
java -version  #查看安装版本
```

![image-20220628093142586](.\image\image-20220628093142586.png)





# PART2. Hadoop环境搭建

安装软件：hadoop2.9.2

## 1. 解压安装包

```shell
tar -zxvf hadoop-2.9.2.tar.gz -C ../app/

vim /etc/profile    #修改环境变量
# HADOOP_HOME
export HADOOP_HOME=/home/app/hadoop-2.9.2
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

source /etc/profile

hadoop version  #查看环境是否生效
```

![image-20220628161550791](.\image\image-20220628161550791.png)



## 2. 配置文件修改

### 2.1 HDFS配置

```shell
cd /home/app/hadoop-2.9.2/etc/hadoop/   #进入到配置文件的路径
```



#### hadoop-env.sh

```shell
vim hadoop-env.sh 

#指定Java路径
export JAVA_HOME=/home/app/jdk1.8
```

![image-20220628162201619](.\image\image-20220628162201619.png)



#### core-site.xml

```shell
vim core-site.xml

#添加内容
#tmp文件需自行创建
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://Pspark:9000</value>
        </property>
        <property>
                <name>io.file.buffer.size</name>
                <value>131072</value>
        </property>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>file:/home/apps/hadoop-2.9.2/tmp</value>
        </property>
        <property>
                <name>hadoop.proxyuser.root.hosts</name>
                <value>*</value>
        </property>
        <property>
                <name>hadoop.proxyuser.root.groups</name>
                <value>*</value>
        </property>

```

#### hdfs-site.xml

```shell
vim hdfs-site.xml

#添加内容
#需要递归创建dfs/name  dfs/data
#dfs.replication为副本数量，一台机器为1，多台机器可增加
        <property>
                <name>dfs.namenode.secondary.http-address</name>
                <value>Pspark:9001</value>
        </property>
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>file:/home/apps/hadoop-2.9.2/dfs/name</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>file:/home/apps/hadoop-2.9.2/dfs/data</value>
        </property>
        <property>
                <name>dfs.replication</name>
                <value>1</value>
        </property>
        <property>
                <name>dfs.webhdfs.enabled</name>
                <value>true</value>
        </property>
        <property>
                <name>dfs.permissions</name>
                <value>false</value>
        </property>
        <property>
                <name>dfs.web.ugi</name>
                <value>supergroup</value>
        </property>

```

#### slaves

```shell
vim slaves

#添加内容
#本文件为datanode与nodemanger的工作指定节点
stu
```



### 2.2 MapReduce配置 

#### mapred-env.sh

```shell
vim mapred-env.sh

#指定MapReduce使用的jdk路径
export JAVA_HOME=/home/app/jdk1.8
```

#### mapred-site.xml

```shell
mv mapred-site.xml.template mapred-site.xml
vim mapred-site.xml

#添加内容
#指定Yarn资源调度框架
       <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.address</name>
                <value>Pspark:10020</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.webapp.address</name>
                <value>Pspark:19888</value>
        </property>

```

### 2.3 Yarn资源框架配置

#### yarn-env.sh

```shell
vim yarn-env.sh

#指定Yarn使用的jdk路径
export JAVA_HOME=/home/app/jdk1.8
```

#### yarn-site.xml

```shell
vim yarn-site.xml

#指定ResourceMnager的master节点信息
<property>
	<name>yarn.resourcemanager.hostname</name>
	<value>Pspark</value>
</property>
<property>
	<name>yarn.nodemanager.aux-services</name>
	<value>mapreduce_shuffle</value>
</property>
```



## 3. Hadoop格式化

```shell
hdfs namenode -format
#出现下图所示内容，为格式化成功
#只能格式化一次
```

<img src=".\image\image-20220628170144939.png" alt="image-20220628170144939" style="zoom:80%;" />



## 4. 启动与关闭

```shell
# 关闭虚拟机前，务必关闭服务进程，否则会进入安全模式
# 启动完成后 jps命令查看进行
start-all.sh
stop-all.sh

```

<img src=".\image\image-20220628170800330.png" alt="image-2022062817080330" style="zoom:80%;" />



## 5. WEB管理页面

```
ip:50070
```

<img src=".\image\image-20220628171014552.png" alt="image-20220628171014552" style="zoom:67%;" />![image-20220628171034233](.\image\image-20220628171034233.png)

![image-20220628171034233](.\image\image-20220628171034233.png)

​	**框内的组件，可在页面上进行文件的上传，迁移工作**
![image-20220628171059896](.\image\image-20220628171059896.png)

# PART3. Hive部署

安装软件：Hive(2.3.7)+MySQL(5.7.26)

```shell
# hive安装包
apache-hive-2.3.7-bin.tar.gz
# MySQL安装包
mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar
# MySQL的JDBC驱动程序
mysql-connector-java-5.1.46.jar
```

<img src=".\image\image-20220629083734200.png" alt="image-20220629083734200" style="zoom:80%;" />



## 1. MySQL安装

### 1.1 删除MariaDB

​	centos7自带的 MariaDB(MariaDB是MySQL的一个分支)，与要安装的MySQL有冲突，需要删除。

```shell
# 查询是否安装了mariadb
rpm -aq | grep mariadb

# 删除mariadb。-e 删除指定的套件；--nodeps 不验证套件的相互关联性
rpm -e --nodeps mariadb-libs
```

### 1.2 安装依赖

```shell
yum install net-tools  perl -y
```



### 1.3 安装MySQL

```shell
# 解压缩
tar xvf mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar

# 依次运行以下命令
rpm -ivh mysql-community-common-5.7.26-1.el7.x86_64.rpm
rpm -ivh mysql-community-libs-5.7.26-1.el7.x86_64.rpm
rpm -ivh mysql-community-client-5.7.26-1.el7.x86_64.rpm
rpm -ivh mysql-community-server-5.7.26-1.el7.x86_64.rpm
```



### 1.4 启动数据库

```shell
systemctl enable mysqld
systemctl start mysqld
```



### 1.5 查找root账户密码

```shell
grep password /var/log/mysqld.log
```

![image-20220629084745966](.\image\image-20220629084745966.png)





### 1.6 修改root密码

​	使用初始密码登录后，修改root密码

<img src=".\image\image-20220629084929566.png" alt="image-20220629084929566" style="zoom:50%;" />

```shell
# 进入MySQL，使用前面查询到的口令
mysql -u root -p

# 设置口令强度；将root口令设置为12345678；刷新
set global validate_password_policy=0;
set password for 'root'@'localhost' =password('12345678');
flush privileges;
```

​	validate_password_policy 密码策略(默认是1)，可配置的值有以下：
​		0 or LOW 仅需需符合密码长度（由参数validate_password_length【默认为8】指定）
​		1 or MEDIUM 满足LOW策略，同时还需满足至少有1个数字，小写字母，大写字母和特殊字符
​		2 or STRONG 满足MEDIUM策略，同时密码不能存在字典文件（dictionaryfile）中

### 1.7 创建hive用户

```sql
-- 创建用户设置口令、授权、刷新
CREATE USER 'hive'@'%' IDENTIFIED BY '12345678';
GRANT ALL ON *.* TO 'hive'@'%';
FLUSH PRIVILEGES;
```



## 2. Hive安装

### 1. 解压

```shell
cd /home/tools
tar -zxvf apache-hive-2.3.7-bin.tar.gz -C ../app/

cd ../app
mv apache-hive-2.3.7-bin/ hive-2.3.7/
```



### 2. 修改环境变量

```shell
vim /etc/profile

# 在 /etc/profile 文件中增加环境变量
export HIVE_HOME=/home/app/hive-2.3.7
export PATH=$PATH:$HIVE_HOME/bin
# 执行并生效
source /etc/profile
```



### 3.修改Hive配置

```xml
cd /home/app/hive-2.3.7/conf
vim hive-site.xml

写入以下内容：

<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<!-- hive元数据的存储位置 -->
	<property>
		<name>javax.jdo.option.ConnectionURL</name>
		<value>jdbc:mysql://stu:3306/hivemetadata?createDatabaseIfNotExist=true&amp;useSSL=false</value>
	</property>
	
	<property>
		<name>javax.jdo.option.ConnectionDriverName</name>
		<value>com.mysql.jdbc.Driver</value>
	</property>
	
	<property>
		<name>javax.jdo.option.ConnectionUserName</name>
		<value>hive</value>	
	</property>
<!-- 连接数据库的口令 -->
	<property>
		<name>javax.jdo.option.ConnectionPassword</name>
		<value>12345678</value>
	</property>

<!-- 数据默认的存储位置(HDFS) -->
	<property>
		<name>hive.metastore.warehouse.dir</name>
		<value>/user/hive/warehouse</value>
	</property>

<!-- 在命令行中，显示当前操作的数据库 -->
	<property>
		<name>hive.cli.print.current.db</name>
		<value>true</value>	
	</property>
	
<!-- 在命令行中，显示数据的表头 -->	
	<property>
		<name>hive.cli.print.header</name>
		<value>true</value>
	</property>

	<property>
		<name>hive.metastore.client.socket.timeout</name>
		<value>3600</value>
	</property>
	
<!-- 操作小规模数据时，使用本地模式，提高效率 -->
	<property>
		<name>hive.exec.mode.local.auto</name>
		<value>true</value>
	</property>
	
</configuration>
```

注：

​	注意jdbc的连接串，如果没有 useSSL=false 会有大量警告

​	在xml文件中 &amp; 表示 &



### 4. 拷贝JDBC驱动

将 mysql-connector-java-5.1.46.jar 拷贝到 $HIVE_HOME/lib

```shell
cp mysql-connector-java-5.1.46.jar ../app/hive-2.3.7/lib/
```

### 5. 初始化数据库

```shell
schematool -dbType mysql -initSchema
```

![image-20220629091311493](.\image\image-20220629091311493.png)

### 6. 启动Hive

```shell
# 启动hive服务之前，请先启动hdfs、yarn的服务
# set hive.exec.mode.local.auto=true;  开启本地运算模式，节约计算资源
hive

hive> show databases;
```

![image-20220629091505311](.\image\image-20220629091505311.png)



### 7. 日志文件修改

​	Hive的log默认存放在 /tmp/root 目录下（root为当前用户名）；这个位置可以修改

```shell
#进入到Hive配置文件目录下
#重命名生效配置文件
mv hive-log4j2.properties.template hive-log4j2.properties

#logs文件需要自行创建
property.hive.log.dir = /home/app/hive-2.3.7/logs
```

![image-20220629092017910](.\image\image-20220629092017910.png)



# PART4. Spark部署

Spark2.4.1   https://archive.apache.org/dist/spark/

## 1. 解压安装包

```shell
cd /home/tools
tar -zxvf spark-2.4.1-bin-hadoop2.7.tgz -C ../app/
mv spark-2.4.1-bin-hadoop2.7/ spark-2.4.1
```

## 2. 设置环境变量

```shell
vi /etc/profile

export SPARK_HOME=/home/apps/spark-2.4.1
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

source /etc/profile
```

## 3. 配置文件

文件位置：$SPARK_HOME/conf

### slaves

```shell
mv slaves.template slaves  #重命名文件，使其生效

vim slaves  
# 写入worker节点的主机名
```

![image-20220629144147900](.\image\image-20220629144147900.png)



### spark-defaults.conf

```shell
mv spark-defaults.conf.template spark-defaults.conf
vim spark-defaults.conf

#写入内容
#创建 HDFS 目录：/spark-log
spark.master	spark://stu:7077
spark.eventLog.enabled	true
spark.eventLog.dir	hdfs://stu:9000/spark-log
spark.serializer	org.apache.spark.serializer.KryoSerializer
spark.driver.memory	4g
```

备注：
	spark.master	定义master节点，缺省端口号 7077
	spark.eventLog.enabled	开启eventLog
	spark.eventLog.dir	eventLog的存放位置
	spark.serializer	一个高效的序列化器
	spark.driver.memory	定义driver内存的大小（缺省1G）



### spark-env.sh

```shell
mv spark-env.sh.template spark-env.sh

vim spark-env.sh
#写入内容
export JAVA_HOME=/home/app/jdk1.8
export HADOOP_HOME=/home/app/hadoop-2.9.2
export HADOOP_CONF_DIR=/home/app/hadoop-2.9.2/etc/hadoop
export SPARK_DIST_CLASSPATH=$(/home/app/hadoop-2.9.2/bin/hadoop classpath)
export SPARK_MASTER_HOST=Pspark
export SPARK_MASTER_PORT=7077
```

## 4. 启动与关闭

```shell
cd /home/app/spark-2.4.1
#默认webUI为8080
sbin/start-all.sh
sbin/stop-all.sh
```



## 5. 替换内置SQL模块【可选】

​	Spark 内嵌的 Hive，在实际使用中, 几乎没有任何人会使用。所以，我们指定使用外置的Hive。

```shell
#Spark 要接管 Hive 需要把 hive-site.xml copy 到conf/目录下
cp /home/app/hive-2.3.7/conf/hive-site.xml /home/app/spark-2.4.1/conf/

#Mysql 的驱动 copy 到 jars/目录下
cp /home/tools/mysql-connector-java-5.1.46.jar /home/app/spark-2.4.1/jars/

#把core-site.xml和hdfs-site.xml 拷贝到conf/目录下
cp /home/app/hadoop-2.9.2/etc/hadoop/hdfs-site.xml /home/app/spark-2.4.1/conf/
cp /home/app/hadoop-2.9.2/etc/hadoop/core-site.xml /home/app/spark-2.4.1/conf/

cd /home/app/spark-2.4.1
bin/spark-shell
spark.sql("show databases").show
#能查询到Hive中的数据库与表内容
```

![image-20220629151007672](.\image\image-20220629151007672.png)

​	如果遇到schema版本问题，不改也没什么问题，改的话，需要在hive-site.xml中添加参数，取消schema版本验证。

```shell
ERROR metastore.ObjectStore: Version information found in metastore differs 2.3.0 from expected schema version 1.2.0. Schema verififcation is disabled hive.metastore.schema.verification so setting version.

FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
```

```xml
<property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
</property>
```

