{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1670ff61-ef11-46b1-b787-3a6e94d93255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 22:22:03.899109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\n",
      "2022-12-18 22:22:03.956810: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\n",
      "/home/ma-user/anaconda3/envs/TensorFlow-2.1/lib/python3.7/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "INFO:root:Using MoXing-v2.1.0.5d9c87c8-5d9c87c8\n",
      "INFO:root:Using OBS-Python-SDK-3.20.9.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "source/20_newsgroup\n",
      "source/20_newsgroup/alt.atheism\n",
      "source/20_newsgroup/comp.graphics\n",
      "source/20_newsgroup/comp.os.ms-windows.misc\n",
      "source/20_newsgroup/comp.sys.ibm.pc.hardware\n",
      "source/20_newsgroup/comp.sys.mac.hardware\n",
      "source/20_newsgroup/comp.windows.x\n",
      "source/20_newsgroup/misc.forsale\n",
      "source/20_newsgroup/rec.autos\n",
      "source/20_newsgroup/rec.motorcycles\n",
      "source/20_newsgroup/rec.sport.baseball\n",
      "source/20_newsgroup/rec.sport.hockey\n",
      "source/20_newsgroup/sci.crypt\n",
      "source/20_newsgroup/sci.electronics\n",
      "source/20_newsgroup/sci.med\n",
      "source/20_newsgroup/sci.space\n",
      "source/20_newsgroup/soc.religion.christian\n",
      "source/20_newsgroup/talk.politics.guns\n",
      "source/20_newsgroup/talk.politics.mideast\n",
      "source/20_newsgroup/talk.politics.misc\n",
      "source/20_newsgroup/talk.religion.misc\n",
      "Found 19996 texts.\n",
      "[[  58  576    3 ...    4  930 2050]\n",
      " [ 221   31  972 ... 2932  552  324]\n",
      " [   0    0    0 ...    3  316 5816]\n",
      " ...\n",
      " [   0    0    0 ...   71  197  514]\n",
      " [   0    0    0 ... 2113 1618 9557]\n",
      " [   0    0    0 ...    3    1 2703]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Shape of data tensor: (19996, 1000)\n",
      "Shape of label tensor: (19996, 20)\n",
      "[[  338   573    35 ...   205    14   287]\n",
      " [    0     0     0 ...  2049  4827  3408]\n",
      " [    0     0     0 ...    26   219     3]\n",
      " ...\n",
      " [    0     0     0 ...   257 11868 14728]\n",
      " [    0     0     0 ... 14564   535    49]\n",
      " [    0     0     0 ...  7025   141    90]]\n",
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "19996\n",
      "Preparing embedding matrix.\n",
      "Training model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 22:22:50.035720: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2022-12-18 22:22:50.063865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 22:22:50.064817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:00:0e.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2022-12-18 22:22:50.064905: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-12-18 22:22:50.064967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-12-18 22:22:50.126366: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2022-12-18 22:22:50.139225: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2022-12-18 22:22:50.278120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-12-18 22:22:50.292538: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-12-18 22:22:50.292597: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-12-18 22:22:50.292753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 22:22:50.293790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 22:22:50.294650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2022-12-18 22:22:50.295867: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-12-18 22:22:50.321599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2599995000 Hz\n",
      "2022-12-18 22:22:50.327777: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5613c8604e60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-18 22:22:50.327806: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-12-18 22:22:50.436158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 22:22:50.437258: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5613c85e6e30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-18 22:22:50.437307: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2022-12-18 22:22:50.437627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 22:22:50.438576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:00:0e.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2022-12-18 22:22:50.438625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-12-18 22:22:50.438638: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-12-18 22:22:50.438661: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2022-12-18 22:22:50.438679: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2022-12-18 22:22:50.438694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-12-18 22:22:50.438709: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-12-18 22:22:50.438721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-12-18 22:22:50.438777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 22:22:50.439669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 22:22:50.440510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2022-12-18 22:22:50.442293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-12-18 22:22:51.537044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-12-18 22:22:51.537088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n",
      "2022-12-18 22:22:51.537100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n",
      "2022-12-18 22:22:51.538932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 22:22:51.539914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-18 22:22:51.540842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15167 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:0e.0, compute capability: 6.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                4112      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                340       \n",
      "=================================================================\n",
      "Total params: 2,238,948\n",
      "Trainable params: 2,238,948\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 15997 samples, validate on 3999 samples\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 22:22:57.437201: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-12-18 22:22:58.227834: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15997/15997 [==============================] - 30s 2ms/sample - loss: 2.8064 - accuracy: 0.1260 - val_loss: 2.4289 - val_accuracy: 0.2101\n",
      "Epoch 2/15\n",
      "15997/15997 [==============================] - 24s 2ms/sample - loss: 2.3017 - accuracy: 0.2334 - val_loss: 2.0392 - val_accuracy: 0.3096\n",
      "Epoch 3/15\n",
      "15997/15997 [==============================] - 24s 2ms/sample - loss: 1.7226 - accuracy: 0.4049 - val_loss: 1.6184 - val_accuracy: 0.4389\n",
      "Epoch 4/15\n",
      "15997/15997 [==============================] - 24s 1ms/sample - loss: 1.4382 - accuracy: 0.5062 - val_loss: 1.5665 - val_accuracy: 0.4769\n",
      "Epoch 5/15\n",
      "15997/15997 [==============================] - 24s 2ms/sample - loss: 1.1078 - accuracy: 0.6257 - val_loss: 1.1186 - val_accuracy: 0.6279\n",
      "Epoch 6/15\n",
      "15997/15997 [==============================] - 24s 2ms/sample - loss: 0.7764 - accuracy: 0.7371 - val_loss: 1.0305 - val_accuracy: 0.6614\n",
      "Epoch 7/15\n",
      "15997/15997 [==============================] - 24s 2ms/sample - loss: 0.5795 - accuracy: 0.8062 - val_loss: 0.8657 - val_accuracy: 0.7279\n",
      "Epoch 8/15\n",
      "15997/15997 [==============================] - 24s 2ms/sample - loss: 0.4519 - accuracy: 0.8489 - val_loss: 0.8656 - val_accuracy: 0.7359\n",
      "Epoch 9/15\n",
      "15997/15997 [==============================] - 24s 2ms/sample - loss: 0.3534 - accuracy: 0.8828 - val_loss: 0.8473 - val_accuracy: 0.7499\n",
      "Epoch 10/15\n",
      "15997/15997 [==============================] - 24s 2ms/sample - loss: 0.2780 - accuracy: 0.9066 - val_loss: 0.8523 - val_accuracy: 0.7519\n",
      "Epoch 11/15\n",
      "15997/15997 [==============================] - 24s 2ms/sample - loss: 0.2418 - accuracy: 0.9191 - val_loss: 0.9527 - val_accuracy: 0.7309\n",
      "Epoch 12/15\n",
      "15997/15997 [==============================] - 24s 2ms/sample - loss: 0.2101 - accuracy: 0.9295 - val_loss: 0.8861 - val_accuracy: 0.7632\n",
      "Epoch 13/15\n",
      "15997/15997 [==============================] - 24s 2ms/sample - loss: 0.1547 - accuracy: 0.9462 - val_loss: 0.9068 - val_accuracy: 0.7732\n",
      "Epoch 14/15\n",
      "15997/15997 [==============================] - 24s 2ms/sample - loss: 0.1288 - accuracy: 0.9534 - val_loss: 0.9484 - val_accuracy: 0.7692\n",
      "Epoch 15/15\n",
      "15997/15997 [==============================] - 24s 2ms/sample - loss: 0.1126 - accuracy: 0.9575 - val_loss: 0.9550 - val_accuracy: 0.7734\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't decrement id ref count (unable to close file, errno = 122, error message = 'Disk quota exceeded')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1691/200613639.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m# 先在虚拟机上保存模型，再将模型拷贝至桶的输出路径下。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0mModel_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mytextlstm_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saved model to disk'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mModel_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;31m# 第二个参数需要根据实验者的桶路径修改\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow-2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \"\"\"\n\u001b[1;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1008\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow-2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    110\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    111\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 112\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow-2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    118\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m       \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlow-2.1/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# Otherwise we get errors in MPI mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_open_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJ_LOCAL\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJ_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_open_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJ_LOCAL\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJ_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.FileID._close_open_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't decrement id ref count (unable to close file, errno = 122, error message = 'Disk quota exceeded')"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import moxing as mox\n",
    "import argparse\n",
    "\n",
    "# BASE_DIR = 'G:\\\\trainingdata'\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='CNN Example')\n",
    "# parser.add_argument('--data_url', type=str, default=\"./Data\",\n",
    "#                     help='path where the dataset is saved')\n",
    "# parser.add_argument('--train_url', type=str, default=\"./Model\", help='model path')\n",
    "# args = parser.parse_args()\n",
    "# # BASE_DIR为训练集根目录，这里设置为桶的dataset目录\n",
    "# BASE_DIR = args.data_url\n",
    "\n",
    "BASE_DIR = 'source'\n",
    "\n",
    "\n",
    "# 文本语料路径\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# 将词变为词向量\n",
    "print('Indexing word vectors.')\n",
    "print(TEXT_DATA_DIR)\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(BASE_DIR, 'glove.6B.100d.txt'), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    print(path)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            # if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "joblib.dump(tokenizer, 'token_result.pkl')\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(data)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "print(data)\n",
    "labels = labels[indices]\n",
    "print(labels)\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "print(data.shape[0])\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        # 从预训练模型的词向量到语料库的词向量映射\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        \n",
    "embedding_layer = Embedding(MAX_NUM_WORDS,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "print('Training model.')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "#请从此开始补充定义CNN和LSTM模型\n",
    "\n",
    "# Your codes\n",
    "model = keras.Sequential([\n",
    "  embedding_layer,\n",
    "  keras.layers.Bidirectional(keras.layers.LSTM(128)),\n",
    "  keras.layers.Dense(16, activation='relu'),\n",
    "  keras.layers.Dense(20, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#补充代码ending\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=15,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "# 先在虚拟机上保存模型，再将模型拷贝至桶的输出路径下。\n",
    "Model_DIR = os.path.join(os.getcwd(), 'mytextlstm_model.h5')\n",
    "model.save(Model_DIR)\n",
    "print('Saved model to disk'+Model_DIR)\n",
    "# 第二个参数需要根据实验者的桶路径修改\n",
    "mox.file.copy_parallel(Model_DIR,'obs://nlp-lab2-cpy/model/mytextlstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791600e8-2751-4940-9ec7-9de294dce0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-2.1",
   "language": "python",
   "name": "tensorflow-2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
