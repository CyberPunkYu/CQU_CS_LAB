{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3bb20b4-6c1d-40ae-9e0a-3107c24cfe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "source/20_newsgroup\n",
      "source/20_newsgroup/alt.atheism\n",
      "source/20_newsgroup/comp.graphics\n",
      "source/20_newsgroup/comp.os.ms-windows.misc\n",
      "source/20_newsgroup/comp.sys.ibm.pc.hardware\n",
      "source/20_newsgroup/comp.sys.mac.hardware\n",
      "source/20_newsgroup/comp.windows.x\n",
      "source/20_newsgroup/misc.forsale\n",
      "source/20_newsgroup/rec.autos\n",
      "source/20_newsgroup/rec.motorcycles\n",
      "source/20_newsgroup/rec.sport.baseball\n",
      "source/20_newsgroup/rec.sport.hockey\n",
      "source/20_newsgroup/sci.crypt\n",
      "source/20_newsgroup/sci.electronics\n",
      "source/20_newsgroup/sci.med\n",
      "source/20_newsgroup/sci.space\n",
      "source/20_newsgroup/soc.religion.christian\n",
      "source/20_newsgroup/talk.politics.guns\n",
      "source/20_newsgroup/talk.politics.mideast\n",
      "source/20_newsgroup/talk.politics.misc\n",
      "source/20_newsgroup/talk.religion.misc\n",
      "Found 19996 texts.\n",
      "[[  58  576    3 ...    4  930 2050]\n",
      " [ 221   31  972 ... 2932  552  324]\n",
      " [   0    0    0 ...    3  316 5816]\n",
      " ...\n",
      " [   0    0    0 ...   71  197  514]\n",
      " [   0    0    0 ... 2113 1618 9557]\n",
      " [   0    0    0 ...    3    1 2703]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Shape of data tensor: (19996, 1000)\n",
      "Shape of label tensor: (19996, 20)\n",
      "[[   0    0    0 ...   17 1621  571]\n",
      " [   0    0    0 ... 5134 2252 6387]\n",
      " [   0    0    0 ... 4244 1731   26]\n",
      " ...\n",
      " [   0    0    0 ...   17   33 9781]\n",
      " [   0    0    0 ...    4  227   68]\n",
      " [   0    0    0 ... 8790 5862   49]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "19996\n",
      "Preparing embedding matrix.\n",
      "Training model.\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_9 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 20)                1300      \n",
      "=================================================================\n",
      "Total params: 2,237,780\n",
      "Trainable params: 2,237,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 15997 samples, validate on 3999 samples\n",
      "Epoch 1/15\n",
      "15997/15997 [==============================] - 7s 448us/sample - loss: 2.9098 - accuracy: 0.0858 - val_loss: 2.4932 - val_accuracy: 0.1953\n",
      "Epoch 2/15\n",
      "15997/15997 [==============================] - 6s 396us/sample - loss: 2.2822 - accuracy: 0.2264 - val_loss: 1.7789 - val_accuracy: 0.3906\n",
      "Epoch 3/15\n",
      "15997/15997 [==============================] - 6s 390us/sample - loss: 1.6352 - accuracy: 0.4296 - val_loss: 1.1838 - val_accuracy: 0.6257\n",
      "Epoch 4/15\n",
      "15997/15997 [==============================] - 6s 396us/sample - loss: 1.1604 - accuracy: 0.5864 - val_loss: 0.9295 - val_accuracy: 0.6889\n",
      "Epoch 5/15\n",
      "15997/15997 [==============================] - 6s 402us/sample - loss: 0.8833 - accuracy: 0.6903 - val_loss: 0.8236 - val_accuracy: 0.7334\n",
      "Epoch 6/15\n",
      "15997/15997 [==============================] - 6s 388us/sample - loss: 0.6862 - accuracy: 0.7660 - val_loss: 0.7824 - val_accuracy: 0.7552\n",
      "Epoch 7/15\n",
      "15997/15997 [==============================] - 6s 403us/sample - loss: 0.5469 - accuracy: 0.8109 - val_loss: 0.7780 - val_accuracy: 0.7647\n",
      "Epoch 8/15\n",
      "15997/15997 [==============================] - 6s 399us/sample - loss: 0.4593 - accuracy: 0.8445 - val_loss: 0.7754 - val_accuracy: 0.7784\n",
      "Epoch 9/15\n",
      "15997/15997 [==============================] - 6s 390us/sample - loss: 0.3679 - accuracy: 0.8760 - val_loss: 0.8135 - val_accuracy: 0.7814\n",
      "Epoch 10/15\n",
      "15997/15997 [==============================] - 6s 390us/sample - loss: 0.3160 - accuracy: 0.8947 - val_loss: 0.8814 - val_accuracy: 0.7832\n",
      "Epoch 11/15\n",
      "15997/15997 [==============================] - 6s 395us/sample - loss: 0.2785 - accuracy: 0.9072 - val_loss: 0.9088 - val_accuracy: 0.7899\n",
      "Epoch 12/15\n",
      "15997/15997 [==============================] - 6s 402us/sample - loss: 0.2428 - accuracy: 0.9159 - val_loss: 0.9514 - val_accuracy: 0.7899\n",
      "Epoch 13/15\n",
      "15997/15997 [==============================] - 6s 391us/sample - loss: 0.2231 - accuracy: 0.9234 - val_loss: 0.9203 - val_accuracy: 0.7959\n",
      "Epoch 14/15\n",
      "15997/15997 [==============================] - 6s 395us/sample - loss: 0.1902 - accuracy: 0.9317 - val_loss: 1.0709 - val_accuracy: 0.7942\n",
      "Epoch 15/15\n",
      "15997/15997 [==============================] - 6s 392us/sample - loss: 0.1964 - accuracy: 0.9336 - val_loss: 1.0410 - val_accuracy: 0.7847\n",
      "3999/3999 [==============================] - 1s 158us/sample - loss: 1.0410 - accuracy: 0.7847\n",
      "Saved model to disk/home/ma-user/work/code/mytextcnn_model.h5\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import moxing as mox\n",
    "import argparse\n",
    "\n",
    "# BASE_DIR = 'G:\\\\trainingdata'\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='CNN Example')\n",
    "# parser.add_argument('--data_url', type=str, default=\"./Data\",\n",
    "#                     help='path where the dataset is saved')\n",
    "# parser.add_argument('--train_url', type=str, default=\"./Model\", help='model path')\n",
    "# args = parser.parse_args()\n",
    "# # BASE_DIR为训练集根目录，这里设置为桶的dataset目录\n",
    "# BASE_DIR = args.data_url\n",
    "\n",
    "BASE_DIR = 'source'\n",
    "\n",
    "\n",
    "# 文本语料路径\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# 将词变为词向量\n",
    "print('Indexing word vectors.')\n",
    "print(TEXT_DATA_DIR)\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(BASE_DIR, 'glove.6B.100d.txt'), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    print(path)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            # if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "joblib.dump(tokenizer, 'token_result.pkl')\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(data)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "print(data)\n",
    "labels = labels[indices]\n",
    "print(labels)\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "print(data.shape[0])\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        # 从预训练模型的词向量到语料库的词向量映射\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        \n",
    "embedding_layer = Embedding(MAX_NUM_WORDS,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "print('Training model.')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "#请从此开始补充定义CNN和LSTM模型\n",
    "\n",
    "# Your codes\n",
    "model = keras.Sequential([\n",
    "  embedding_layer,\n",
    "  keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "  keras.layers.MaxPooling1D(5),\n",
    "  keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "  keras.layers.MaxPooling1D(5),\n",
    "  keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "  keras.layers.GlobalMaxPooling1D(),\n",
    "  keras.layers.Dense(64, activation='relu'),\n",
    "  keras.layers.Dropout(0.5),\n",
    "  keras.layers.Dense(20, activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#补充代码ending\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=15,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "# 先在虚拟机上保存模型，再将模型拷贝至桶的输出路径下。\n",
    "Model_DIR = os.path.join(os.getcwd(), 'mytextcnn_model.h5')\n",
    "model.save(Model_DIR)\n",
    "print('Saved model to disk'+Model_DIR)\n",
    "# 第二个参数需要根据实验者的桶路径修改\n",
    "mox.file.copy_parallel(Model_DIR,'obs://nlp-lab2-cpy/model/mytextcnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7049491-2ba4-41ee-a9ce-71ad7d99e1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-2.1",
   "language": "python",
   "name": "tensorflow-2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
